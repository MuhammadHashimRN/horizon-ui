# Gesture Dataset

## Structure

```
dataset/
├── raw/          # Raw video recordings per gesture class
│   ├── point/
│   ├── pinch/
│   ├── fist/
│   ├── open_palm/
│   ├── swipe_left/
│   ├── swipe_right/
│   ├── pinch_spread/
│   ├── pinch_close/
│   ├── two_finger_scroll/
│   ├── thumbs_up/
│   └── none/
├── processed/    # Extracted landmark sequences (.npy files)
│   └── <same classes as raw>/
└── augmented/    # Augmented data (generated by augment_data.py)
```

## Format

Each `.npy` file in `processed/` contains a NumPy array of shape `(T, 63)`:
- `T` = number of frames in the sequence
- `63` = 21 landmarks × 3 coordinates (x, y, z)

Coordinates are normalized to [0, 1] by MediaPipe.

## Collection

Use the collection script:

```bash
python scripts/collect_gesture_data.py --gesture pinch --output training/dataset/raw/pinch/ --samples 20
```

## Processing

Extract landmarks from raw video (to be implemented):

```bash
python scripts/process_raw_data.py --input dataset/raw --output dataset/processed
```
